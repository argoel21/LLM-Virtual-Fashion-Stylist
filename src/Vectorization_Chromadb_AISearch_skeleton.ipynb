{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gdown\n",
        "!gdown \"1igAuIEW_4h_51BG1o05WS0Q0-Cp17_-t&confirm=t\"\n",
        "!unzip data\n"
      ],
      "metadata": {
        "id": "QXBr2IQfQ5VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U fashion-clip"
      ],
      "metadata": {
        "id": "4_uc1HK3Q91X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "#sys.path.append(\"fashion-clip/\")\n",
        "from fashion_clip.fashion_clip import FashionCLIP\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import *\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "J3adElWVQ-Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "fclip = FashionCLIP('fashion-clip')"
      ],
      "metadata": {
        "id": "v_4ZBLmeQ-Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles = pd.read_csv(\"data_for_fashion_clip/articles.csv\")\n",
        "\n",
        "# drop items that have the same description\n",
        "subset = articles.drop_duplicates(\"detail_desc\").copy()\n",
        "\n",
        "# remove items of unkown category\n",
        "subset = subset[~subset[\"product_group_name\"].isin([\"Unknown\"])]\n",
        "\n",
        "# FashionCLIP has a limit of 77 tokens, let's play it safe and drop things with more than 40 tokens\n",
        "subset = subset[subset[\"detail_desc\"].apply(lambda x : 4 < len(str(x).split()) < 40)]\n",
        "\n",
        "# We also drop products types that do not occur very frequently in this subset of data\n",
        "most_frequent_product_types = [k for k, v in dict(Counter(subset[\"product_type_name\"].tolist())).items() if v > 10]\n",
        "subset = subset[subset[\"product_type_name\"].isin(most_frequent_product_types)]\n",
        "\n",
        "# lots of data here, but we will just use only descriptions and a couple of other columns\n",
        "subset.head(10)"
      ],
      "metadata": {
        "id": "B2HwKDaaRGXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorization of Clothing Inputs"
      ],
      "metadata": {
        "id": "LMhEMOVkr2Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Load FashionCLIP\n",
        "from fashion_clip.fashion_clip import FashionCLIP\n",
        "fclip = FashionCLIP('fashion-clip')\n"
      ],
      "metadata": {
        "id": "aed_aWKNRGrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Chromadb"
      ],
      "metadata": {
        "id": "HU40yreVuFFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb\n",
        "\n",
        "import chromadb\n",
        "\n",
        "client = chromadb.Client()\n",
        "collection = client.get_or_create_collection(name="clothing_text_embeddings")\n",
        "\n",
        "for idx, row in articles.iterrows():\n",
        "    try:\n",
        "        # Load and preprocess image\n",
        "        image_path = f\"images/{row['article_id']}.jpg\"  # adjust path based on your folder structure\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Generate image embedding\n",
        "        embedding = fclip.encode_images([image], batch_size = 1)[0].tolist()  # Convert to list for ChromaDB\n",
        "\n",
        "        # Construct metadata\n",
        "        metadata = {\n",
        "            \"category\": row[\"product_type_name\"],\n",
        "            \"season\": str(row.get(\"season\", \"unknown\")),  # If season exists\n",
        "            \"gender\": row.get(\"index_name\", \"unknown\")\n",
        "        }\n",
        "\n",
        "        # Add to ChromaDB\n",
        "        collection.add(\n",
        "            embeddings=[embedding],\n",
        "            documents=[row[\"detail_desc\"]],\n",
        "            metadatas=[metadata],\n",
        "            ids=[f\"item_{row['article_id']}\"]\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping item {row['article_id']} due to error: {e}\")\n",
        
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YFdh-WHWRG6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Querying the db"
      ],
      "metadata": {
        "id": "thxSkzbOzCkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "O2gztjMeuCWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search with an uploaded image\n",
        "query_image = Image.open(\"user_upload.jpg\").convert(\"RGB\")\n",
        "query_embedding = fclip.encode_images([query_image], batch_size = 1)[0].tolist()\n",
        "\n",
        "results = collection.query(\n",
        "    query_embeddings=[query_embedding],\n",
        "    n_results=5,\n",
        "    where={\"season\": \"summer\", \"gender\": \"Ladieswear\"}  # Optional metadata filtering\n",
        ")"
      ],
      "metadata": {
        "id": "tUb3aRlMzJpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** If we Dont Have Image Data Avaiable, can also use encode_text..."
      ],
      "metadata": {
        "id": "laEfqL3pz8ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fashion_clip.fashion_clip import FashionCLIP\n",
        "import chromadb\n",
        "import pandas as pd\n",
        "\n",
        "# Load FashionCLIP\n",
        "fclip = FashionCLIP('fashion-clip')\n",
        "\n",
        "# Load and filter your dataset (if not done already)\n",
        "df = pd.read_csv(\"data_for_fashion_clip/articles.csv\")\n",
        "df = df.drop_duplicates(\"detail_desc\")\n",
        "df = df[~df[\"product_group_name\"].isin([\"Unknown\"])]\n",
        "df = df[df[\"detail_desc\"].apply(lambda x: 4 < len(str(x).split()) < 40)]\n",
        "most_frequent = df[\"product_type_name\"].value_counts()[df[\"product_type_name\"].value_counts() > 10].index\n",
        "df = df[df[\"product_type_name\"].isin(most_frequent)].copy()\n",
        "\n",
        "# Initialize ChromaDB\n",
        "client = chromadb.Client()\n",
        "collection = client.get_or_create_collection(name=\"clothing_text_embeddings\")\n",
        "\n",
        "# Batch loop: Encode and insert into ChromaDB\n",
        "for idx, row in df.iterrows():\n",
        "    try:\n",
        "        text = row[\"detail_desc\"]\n",
        "        text_embedding = fclip.encode_text([text], batch_size = 1)[0].tolist()\n",
        "\n",
        "        metadata = {\n",
        "            \"category\": row[\"product_type_name\"],\n",
        "            \"group\": row[\"product_group_name\"],\n",
        "            \"gender\": row[\"index_name\"],\n",
        "            \"id\": str(row[\"article_id\"])\n",
        "        }\n",
        "\n",
        "        collection.add(\n",
        "            embeddings=[text_embedding],\n",
        "            documents=[text],\n",
        "            metadatas=[metadata],\n",
        "            ids=[f\"text_{row['article_id']}\"]\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error embedding item {row['article_id']}: {e}\")"
      ],
      "metadata": {
        "id": "85BmLJcJ0Cmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def search_clothes(user_query):\n",
        "    filters = parse_query(user_query)  # GPT or rule-based\n",
        "    vector = get_embedding(user_query)\n",
        "    results = search_db(vector, filters)\n",
        "    return format_results(results)\n",
        "\n",
        "gr.Interface(fn=search_clothes, inputs=\"text\", outputs=\"html\").launch()\n"
      ],
      "metadata": {
        "id": "zuXfNaDQ0MSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI Search Interface"
      ],
      "metadata": {
        "id": "0jA--Cja3ank"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from fashion_clip.fashion_clip import FashionCLIP\n",
        "import chromadb\n",
        "\n",
        "# Load everything\n",
        "fclip = FashionCLIP('fashion-clip')\n",
        "client = chromadb.Client()\n",
        "collection = client.get_or_create_collection(\"clothing_text_embeddings\")"
      ],
      "metadata": {
        "id": "FJA8MUYK3cqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy parser for now (can use GPT later)\n",
        "def parse_query(text):\n",
        "    text = text.lower()\n",
        "    filters = {}\n",
        "    if \"summer\" in text: filters[\"season\"] = \"summer\"\n",
        "    if \"winter\" in text: filters[\"season\"] = \"winter\"\n",
        "    if \"dress\" in text: filters[\"category\"] = \"Dress\"\n",
        "    return filters\n",
        "\n",
        "# Search function\n",
        "def search_clothes(user_query):\n",
        "    try:\n",
        "      # encodes our query into parsable, vector emebddings\n",
        "        query_vector = fclip.encode_text([user_query], batch_size = 1)[0].tolist()\n",
        "        # searches for known key words\n",
        "        filters = parse_query(user_query)\n",
        "\n",
        "        # Vector Search; searches for vectors closest to the query vector\n",
        "        results = collection.query(\n",
        "            query_embeddings=[query_vector],\n",
        "            n_results=5,\n",
        "            where=filters if filters else None\n",
        "        )\n",
        "\n",
        "        # Format results\n",
        "        items = results[\"documents\"][0]\n",
        "        metadatas = results[\"metadatas\"][0]\n",
        "        display = \"\"\n",
        "        for item, meta in zip(items, metadatas):\n",
        "            display += f\"<b>{meta.get('category', 'Item')}</b>: {item}<br><i>{meta}</i><br><br>\"\n",
        "        return display or \"No results found.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Launch app\n",
        "gr.Interface(fn=search_clothes, inputs=\"text\", outputs=\"html\", title=\"AI Fashion Search\").launch()\n"
      ],
      "metadata": {
        "id": "PvrgYrvK3k5I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
